(!! add stopwords into Compressed, remove them in both runs)
(!! remove punctuations)

created an array _docBody[][], in which _docBody[i] is an array of 40 integers.
Every two represent a term and its count, so it records 20 top terms.

The assignments on _docBody is finished in the second run, because "uniqueTerms" has what we need,
HashMap<Integer, Vector<Integer>> uniqueTerms use key to represent the term,
and value to represent offset of occurrences,

so we can have a HashMap<Integer,Integer> hm from it that key is term, value is number of occurrences

then we sort hm and extract top 20 terms and copy them to _docBody[i],
in which i is the current docid, (we can get that with _document.size()-1).

So after this, we have an array that knows what words are top in every doc.